---
layout: page
---

# About Me

Here is **Yong Dai (ä»£å‹‡)**.<br>

---

## Brief Bio
As a dedicated AI researcher, my work bridges the gap between large language models and the complex, multi-modal world we live in. My journey began at the University of Electronic Science and Technology of China (UESTC), where I completed my Ph.D. under the guidance of Professor Zenglin Xu. During my doctoral program, I also had the valuable opportunity to learn and grow as a research intern at Microsoft. This experience, combined with my subsequent role as a researcher at Tencent AI Lab, solidified my expertise in harnessing large-scale models for a wide spectrum of downstream tasks.

Recently, my focus has shifted to what I see as the next frontier: **multi-modality** and **web agents**. I am deeply fascinated by the pursuit of a unified paradigmâ€”a single, elegant model that can perceive, reason, and create across text, images, and other data formats. My long-term vision is to contribute to the development of a true "World Model." By integrating such a powerful generative and understanding system with autonomous agent technology, I aim to play a part in building the foundation for **Artificial General Intelligence (AGI)** and creating systems that can truly benefit humanity.

---

## Experience

- 05/21 - 04/24: Research Intern and Researcher, Tencent AI Lab
- 11/20 - 04/21: Visiting student, Westlake University
- 09/19 - 10/20: Research Intern, Microsoft STCA nlpg
- 09/18 - 08/19: Project leader, cooperation project with Nuance

## Research Interests

<img src="images/interest.jpg" alt="Research Interests" width="600"/>

## Project

- 
- 
- 

---

| Year | Content |
|------|---------|
| <img src="images/news.png" width="20" style="vertical-align:middle; margin-right:4px;"/> **Feb 2025** | ðŸŽ‰ Paper accepted to <span style="color:red">ACM MM 2025</span> |
| <img src="images/news.png" width="20" style="vertical-align:middle; margin-right:4px;"/> **Nov 2024** | Released paper *MME-finance*: A multimodal finance benchmark |
| <img src="images/news.png" width="20" style="vertical-align:middle; margin-right:4px;"/> **Oct 2024** | Released paper *TimeCNN*: Refining Cross-Variable Interaction |
| <img src="images/news.png" width="20" style="vertical-align:middle; margin-right:4px;"/> **Sep 2024** | ðŸŽ‰ Two papers accepted to <span style="color:red">NeurIPS 2024</span> |
| <img src="images/news.png" width="20" style="vertical-align:middle; margin-right:4px;"/> **Sep 2024** | ðŸŽ‰ One paper accepted to <span style="color:red">EMNLP 2024 Findings</span> |
| <img src="images/news.png" width="20" style="vertical-align:middle; margin-right:4px;"/> **Jun 2024** | ðŸŽ‰ Three papers accepted to <span style="color:red">ACL 2024</span> |
| <img src="images/news.png" width="20" style="vertical-align:middle; margin-right:4px;"/> **Dec 2023** | Released evaluation paper *TencentLLMEval* |
| <img src="images/news.png" width="20" style="vertical-align:middle; margin-right:4px;"/> **Dec 2023** | Released reward adaptation paper *Everyone deserves a reward* |
| <img src="images/news.png" width="20" style="vertical-align:middle; margin-right:4px;"/> **Jun 2023** | ðŸŽ‰ Paper *SkillNet-X* accepted to <span style="color:red">ICASSP 2024</span> |
| <img src="images/news.png" width="20" style="vertical-align:middle; margin-right:4px;"/> **Dec 2022** | ðŸŽ‰ Paper *Federated Learning + PLMs* accepted to <span style="color:red">Findings of ACL 2023</span> |
| <img src="images/news.png" width="20" style="vertical-align:middle; margin-right:4px;"/> **Oct 2022** | ðŸŽ‰ Paper *Prompt-based Constrained Clustering* accepted to <span style="color:red">Findings of EMNLP 2022</span> |
| <img src="images/news.png" width="20" style="vertical-align:middle; margin-right:4px;"/> **Aug 2022** | Released technical report *Effidit: Your AI Writing Assistant* |
| <img src="images/news.png" width="20" style="vertical-align:middle; margin-right:4px;"/> **Aug 2022** | Released paper *Chinese BERT Error Detection* |
| <img src="images/news.png" width="20" style="vertical-align:middle; margin-right:4px;"/> **Mar 2022** | Released paper *MarkBERT* |
| <img src="images/news.png" width="20" style="vertical-align:middle; margin-right:4px;"/> **Mar 2022** | ðŸŽ‰ Paper *Whole Word Masking* accepted to <span style="color:red">Findings of ACL 2022</span> |
| <img src="images/news.png" width="20" style="vertical-align:middle; margin-right:4px;"/> **Mar 2022** | ðŸŽ‰ Paper *Chinese GPT for Pinyin Input* accepted to <span style="color:red">ACL 2022</span> |
| <img src="images/news.png" width="20" style="vertical-align:middle; margin-right:4px;"/> **Jan 2022** | ðŸŽ‰ Paper *Graph Fusion Network* accepted to <span style="color:red">KBS</span> |
| <img src="images/news.png" width="20" style="vertical-align:middle; margin-right:4px;"/> **Sep 2021** | ðŸŽ‰ Paper *Unsupervised Sentiment Analysis* accepted to <span style="color:red">CC</span> |
| <img src="images/news.png" width="20" style="vertical-align:middle; margin-right:4px;"/> **Oct 2020** | ðŸŽ‰ Paper *Contextualize KBs with Transformer* accepted to <span style="color:red">EMNLP 2021 (Oral)</span> |
| <img src="images/news.png" width="20" style="vertical-align:middle; margin-right:4px;"/> **Apr 2020** | ðŸŽ‰ Paper *Adversarial Training for Sentiment Analysis* accepted to <span style="color:red">AAAI 2020</span> |
