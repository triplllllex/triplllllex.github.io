---
layout: page
---

# About Me

Here is **Yong Dai (ä»£å‹‡)**.<br>

---

## Brief Bio
As a dedicated AI researcher, my work bridges the gap between large language models and the complex, multi-modal world we live in. My journey began at the University of Electronic Science and Technology of China (UESTC), where I completed my Ph.D. under the guidance of Professor Zenglin Xu. During my doctoral program, I also had the valuable opportunity to learn and grow as a research intern at Microsoft. This experience, combined with my subsequent role as a researcher at Tencent AI Lab, solidified my expertise in harnessing large-scale models for a wide spectrum of downstream tasks.

Recently, my focus has shifted to what I see as the next frontier: **multi-modality** and **web agents**. I am deeply fascinated by the pursuit of a unified paradigmâ€”a single, elegant model that can perceive, reason, and create across text, images, and other data formats. My long-term vision is to contribute to the development of a true "World Model." By integrating such a powerful generative and understanding system with autonomous agent technology, I aim to play a part in building the foundation for **Artificial General Intelligence (AGI)** and creating systems that can truly benefit humanity.

---

## Experience

- 05/21 - 04/24: Research Intern and Researcher, Tencent AI Lab
- 11/20 - 04/21: Visiting student, Westlake University
- 09/19 - 10/20: Research Intern, Microsoft STCA nlpg
- 09/18 - 08/19: Project leader, cooperation project with Nuance

## Research Interests

<img src="images/interest.jpg" alt="Research Interests" width="600"/>

## Project

- 
- 
- 

---

| year | content |
|------|---------|
| **Feb 2025** | <img src="images/news.png" width="20"/> ðŸŽ‰ Paper accepted to <span style="color:red">ACM MM 2025</span> |
| **Nov 2024** | <img src="images/news.png" width="20"/> Released paper *MME-finance*: A multimodal finance benchmark |
| **Oct 2024** | <img src="images/news.png" width="20"/> Released paper *TimeCNN*: Refining Cross-Variable Interaction |
| **Sep 2024** | <img src="images/news.png" width="20"/> ðŸŽ‰ Two papers accepted to <span style="color:red">NeurIPS 2024</span> |
| **Sep 2024** | <img src="images/news.png" width="20"/> ðŸŽ‰ One paper accepted to <span style="color:red">EMNLP 2024 Findings</span> |
| **Jun 2024** | <img src="images/news.png" width="20"/> ðŸŽ‰ Three papers accepted to <span style="color:red">ACL 2024</span> |
| **Dec 2023** | <img src="images/news.png" width="20"/> Released evaluation paper *TencentLLMEval* |
| **Dec 2023** | <img src="images/news.png" width="20"/> Released reward adaptation paper *Everyone deserves a reward* |
| **Jun 2023** | <img src="images/news.png" width="20"/> ðŸŽ‰ Paper *SkillNet-X* accepted to <span style="color:red">ICASSP 2024</span> |
| **Dec 2022** | <img src="images/news.png" width="20"/> ðŸŽ‰ Paper *Federated Learning + PLMs* accepted to <span style="color:red">Findings of ACL 2023</span> |
| **Oct 2022** | <img src="images/news.png" width="20"/> ðŸŽ‰ Paper *Prompt-based Constrained Clustering* accepted to <span style="color:red">Findings of EMNLP 2022</span> |
| **Aug 2022** | <img src="images/news.png" width="20"/> Released technical report *Effidit: Your AI Writing Assistant* |
| **Aug 2022** | <img src="images/news.png" width="20"/> Released paper *Chinese BERT Error Detection* |
| **Mar 2022** | <img src="images/news.png" width="20"/> Released paper *MarkBERT* |
| **Mar 2022** | <img src="images/news.png" width="20"/> ðŸŽ‰ Paper *Whole Word Masking* accepted to <span style="color:red">Findings of ACL 2022</span> |
| **Mar 2022** | <img src="images/news.png" width="20"/> ðŸŽ‰ Paper *Chinese GPT for Pinyin Input* accepted to <span style="color:red">ACL 2022</span> |
| **Jan 2022** | <img src="images/news.png" width="20"/> ðŸŽ‰ Paper *Graph Fusion Network* accepted to <span style="color:red">KBS</span> |
| **Sep 2021** | <img src="images/news.png" width="20"/> ðŸŽ‰ Paper *Unsupervised Sentiment Analysis* accepted to <span style="color:red">CC</span> |
| **Oct 2020** | <img src="images/news.png" width="20"/> ðŸŽ‰ Paper *Contextualize KBs with Transformer* accepted to <span style="color:red">EMNLP 2021 (Oral)</span> |
| **Apr 2020** | <img src="images/news.png" width="20"/> ðŸŽ‰ Paper *Adversarial Training for Sentiment Analysis* accepted to <span style="color:red">AAAI 2020</span> |


